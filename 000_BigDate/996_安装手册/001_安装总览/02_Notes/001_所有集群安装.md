大数据安装



# A 快照基础

![image.png](https://cdn.nlark.com/yuque/0/2021/png/10367893/1611902927039-2be5afd7-54a7-4398-98e2-d92c0a77eb92.png)

时间同步：yum info ntp &&  ntpdate cn.ntp.org.cn

## A1 | 初始化（init）

### 1.1 装虚拟机

#### 调整时区

上海

#### 磁盘分区（50G）

/boot：526

swap：2048

/：剩下的

#### root密码

root

### 1.2 配网

#### 配置路径

```
vi /etc/sysconfig/network-scripts/ifcfg-ens33
```

#### 内容

--修改

ONBOOT=yes  

BOOTPROTO=static  //静态网络IP  dhcp 动态获取网络IP

--添加

IPADDR=192.168.10.100

NETMASK=255.255.255.0

GATEWAY=192.168.10.2

DNS1=114.114.114.114

--删除

UUID

#### 重启网卡

ip addr

systemctl restart network

ping www.baidu.com



### 1.3 防火墙

#### 关闭本次防火墙

systemctl stop firewalld

#### 禁用防火墙

systemctl disable firewalld



### 1.4 软件安装限制

#### 文件

vi /etc/selinux/config

#### 修改

SELINUX=disabled

#### 重启

reboot

### 1.5 改主机名

vi /etc/hostname

## A2 | ssh免密钥（keys）

### 2.1 克隆两个主机

修改两个主机的【23】

#### 1.主机名

【vim /etc/hostname】

#### 2.ip 

【vim `/etc/sysconfig/network-scripts/ifcfg-ens33`】

【systemctl restart network】



### 2.2 生成密钥【123】

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

### 2.3 传递密钥【123】

ssh-copy-id -i ~/.ssh/id_rsa.pub **root@192.168.58.201（目标ip）
**

### 2.4 主机名与host校验【123】

ssh localhost

**修改文件：**

vim /etc/ssh/ssh_config

**在其最后添加：**

StrictHostKeyChecking no

UserKnownHostsFile /dev/null

UserKnownHostsFile ~/.ssh/known_hostname

### 2.5 添加域名【123】

**修改文件**

vim /etc/host

**修改内容（三个主机名）**

192.168.10.110 bdp1

192.168.10.111 bdp2

192.168.10.112 bdp3



## A3 | 基础（base）

### 3.1 安装wget

【yum install wget -y】

### 3.2 修改yum源

【mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup】

【wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo】

【yum clean all】

【yum makecache】

### 3.3 安装常用软件

【yum install man man-pages ntp vim lrzsz zip unzip telnet perl net-tools -y】

### 3.4 同步系统时间

【yum info ntp &&  ntpdate cn.ntp.org.cn】

## A4 | Nginx（电商会用）

### 4.1 解编安启

- 将安装包上传到Linux虚拟机
- 解压，并把Nginx存放到指定目录
  - tar -zxf nginx-1.8.1.tar.gz
- 需要安装nginx依赖的软件
  - yum install gcc pcre-devel zlib-devel openssl-devel -y
- 配置安装路径
  - ./configure --prefix=/opt/xxx/nginx-1.8.1
- 开始编译并安装nginx
  - make && make install
- 开启Ngxin
  - cd /opt/xxx/nginx-1.8.1/sbin
  - ./nginx
  - http://192.168.58.100/

### 4.2 集群

- 节点分布

  - | 节点\角色 | Nginx | Tomcat | JDK  |
    | --------- | ----- | ------ | ---- |
    | basenode  | *     |        | *    |
    | node01    |       | *      | *    |
    | node02    |       | *      | *    |
    | node03    |       | *      | *    |

- BaseNode修改配置文件

  - ```
    #user  nobody;
    worker_processes  1;
    
    events {
        worker_connections  1024;
    }
    
    http {
        include       mime.types;
        default_type  application/octet-stream;
        sendfile        on;
        keepalive_timeout  0;
    
        upstream xxx {
    		server 192.168.58.101:8080;
    		server 192.168.58.102:8080;
    		server 192.168.58.103:8080;
        }
    
        server {
            listen       80;
            server_name  localhost;
            location / {
    			proxy_pass http://xxx;
            }
        }
    }
    ```

- Tomcat配置信息

  - vi /opt/xxx/apache-tomcat--8.5.47/webapps/ROOT/index.jsp

  - ```
    <%@ page pageEncoding="UTF-8" contentType="text/html; charset=UTF-8" %>
    <!DOCTYPE html>
    <html lang="en">
        <head>  
            <meta charset="UTF-8" />
            <title>xxxxx</title>
            <link href="favicon.ico" rel="icon" type="image/x-icon" />
            <link href="favicon.ico" rel="shortcut icon" type="image/x-icon" />
        </head> 
        <body>  
            <h1>welcome to xxxxx</h1> 
            <h2>Server:<%=request.getLocalAddr()  %></h2> 
            <h2>Port:<%=request.getLocalPort()  %></h2> 
            <h2>Client:<%=request.getRemoteAddr() %></h2> 
            <h2>Session:<%=session.getId() %></h2> 
            <hr/>   
            <img src="/static/img/qbl.jpg" height="200px" />
        </body>
    </html>
    ```

- 启动Tomcat和Nginx

  - 访问Nginx，会依次将Tomcat页面显示到也面


# B zookeeper

## B1 | 前提

首先将三台虚拟机切换到相互免秘钥快照（keysfree）

## B2 | 安装

- 上传Zookeeper,解压，拷贝

  - ```
    [root@node01 ~]# tar -zxvf zookeeper-3.4.5.tar.gz
    [root@node01 ~]# mv zookeeper-3.4.5 /opt/lzj/
    ```

- 修改配置文件

  - ```
    [root@node01 conf]# cd /opt/lzj/zookeeper-3.4.5/conf/
    [root@node01 conf]# cp zoo_sample.cfg zoo.cfg
    [root@node01 conf]# vim zoo.cfg
    ```

  - ```
    # the directory where the snapshot is stored.
    dataDir=/var/lzj/zookeeper
    # the port at which the clients will connect
    clientPort=2181
    
    # 设置服务器内部通信的地址和zk集群的节点
    server.1=node01:2888:3888
    server.2=node02:2888:3888
    server.3=node03:2888:3888
    ```

- 创建myid

  - ```shell
    [123]mkdir -p /var/lzj/zookeeper
    [123]touch /var/lzj/zookeeper/myid
    [1] echo 1 > /var/lzj/zookeeper/myid
    [2] echo 2 > /var/lzj/zookeeper/myid
    [3] echo 3 > /var/lzj/zookeeper/myid
    ```

- 拷贝Zookeeper

  - ```
    [root@node01 lzj]scp -r zookeeper-3.4.5 root@node02:/opt/lzj/
    [root@node01 lzj]scp -r zookeeper-3.4.5 root@node03:/opt/lzj/
    ```

- 设置环境变量

  - vim /etc/profile

  - ```
    export ZOOKEEPER_HOME=/opt/lzj/zookeeper-3.4.5
    export PATH=$ZOOKEEPER_HOME/bin:$PATH
    ```

  - [1]scp /etc/profile root@node02:/etc/profile

  - [1]scp /etc/profile root@node03:/etc/profile

  - [123] source /etc/profile

- 开启集群

  - [123] zkServer.sh start
  - [123] zkServer.sh status
  - [123] zkServer.sh stop

  - 关机拍摄快照

    - shutdown -h now 

# C HadoopHA

搭建高可用集群

|        | NameNode-1 | NameNode-2 | DataNode | Zookeeper | ZKFC | JouralNode |
| ------ | ---------- | ---------- | -------- | --------- | ---- | ---------- |
| Node01 | *          |            | *        | *         | *    | *          |
| Node02 |            | *          | *        | *         | *    | *          |
| Node03 |            |            | *        | *         |      | *          |

## C1 | 准备安装环境

- ```
  [root@node01 ~]# tar -zxvf hadoop-3.1.2.tar.gz
  [root@node01 ~]# mv hadoop-3.1.2 /opt/lzj/
  [root@node01 ~]# cd /opt/lzj/hadoop-3.1.2/etc/hadoop/
  ```

## C2 | 修改集群环境

- [root@node01 hadoop]# vim hadoop-env.sh

  - ```shell
    ##直接在文件的最后添加
    export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64
    export HDFS_NAMENODE_USER=root
    export HDFS_DATANODE_USER=root
    export HDFS_ZKFC_USER=root
    export HDFS_JOURNALNODE_USER=root
    export YARN_RESOURCEMANAGER_USER=root
    export YARN_NODEMANAGER_USER=root
    ```

## C3 | 修改配置文件

- [root@node01 hadoop]# vim core-site.xml

  - ```xml
    <property>
    <name>fs.defaultFS</name>
    <value>hdfs://hdfs-lzj</value>
    </property>
    <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/lzj/hadoop/ha</value>
    </property>
    <property>
    <name>hadoop.http.staticuser.user</name>
    <value>root</value>
    </property>
    <property>
    <name>ha.zookeeper.quorum</name>
    <value>node01:2181,node02:2181,node03:2181</value>
    </property>
    ```

- [root@node01 hadoop]# vim hdfs-site.xml

  - ```xml
    <property>
    <name>dfs.nameservices</name>
    <value>hdfs-lzj</value>
    </property>
    <property>
    <name>dfs.ha.namenodes.hdfs-lzj</name>
    <value>nn1,nn2</value>
    </property>
    <property>
    <name>dfs.namenode.rpc-address.hdfs-lzj.nn1</name>
    <value>node01:8020</value>
    </property>
    <property>
    <name>dfs.namenode.rpc-address.hdfs-lzj.nn2</name>
    <value>node02:8020</value>
    </property>
    <property>
    <name>dfs.namenode.http-address.hdfs-lzj.nn1</name>
    <value>node01:9870</value>
    </property>
    <property>
    <name>dfs.namenode.http-address.hdfs-lzj.nn2</name>
    <value>node02:9870</value>
    </property>
    <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://node01:8485;node02:8485;node03:8485/hdfs-lzj</value>
    </property>
    <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/var/lzj/hadoop/ha/qjm</value>
    </property>
    <property>
    <name>dfs.client.failover.proxy.provider.hdfs-lzj</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
    <value>shell(true)</value>
    </property>
    <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
    </property>
    <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
    </property>
    <property>
    <name>dfs.replication</name>
    <value>2</value>
    </property>
    ```

- [root@node01 hadoop]# vim workers

  - ```
    node01
    node02
    node03
    ```

## C4 | 拷贝分发软件

- 将配置好的软件分发到其他主机

  - ```shell
    [root@node02 ~]# scp -r root@node01:/opt/lzj/hadoop-3.1.2 /opt/lzj/
    [root@node03 ~]# scp -r root@node01:/opt/lzj/hadoop-3.1.2 /opt/lzj/
    ```

## C5 | 修改环境变量

- [root@node01 hadoop]# vim /etc/profile

  - ```shell
    export HADOOP_HOME=/opt/lzj/hadoop-3.1.2
    export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
    ```

- 将环境变量拷贝到其他主机

  - ```shell
    [root@node01 lzj]# scp /etc/profile root@node02:/etc/profile
    [root@node01 lzj]# scp /etc/profile root@node03:/etc/profile
    ```

- 重新加载三台服务器的环境变量

  - ```
    【123】# source /etc/profile
    ```

## C6 | 首先启动Zookeeper

- 【123】zkServer.sh start
- 【123】zkServer.sh status

## C7 | 启动JournalNode

- 【123】 hdfs --daemon start journalnode
- ![image-20201103232040951](K:/100%2520-%2520%25E5%25AD%25A6%25E4%25B9%25A0/105_sxtbd/one/%25E5%25A4%25A7%25E6%2595%25B0%25E6%258D%25AE%25E6%2596%2587%25E6%25A1%25A3/0201Hadoop-HDFS/006_document/Hadoop-HDFS03.assets/image-20201103232040951.png)

## C8 | 格式化NameNode

（注意不能重复）

- [root@node01 lzj]# hdfs namenode -format
- [root@node01 lzj]# hdfs --daemon start namenode
- [root@node02 lzj]# hdfs namenode -bootstrapStandby
- [root@node01 lzj]# hdfs zkfc -formatZK
- [root@node01 lzj]# start-dfs.sh

## C9 | 测试集群

- http://node01:9870
- http://node02:9870
- [root@node01 ~]# hdfs dfs -mkdir -p /lzj
- [root@node01 ~]# hdfs dfs -put zookeeper-3.4.5.tar.gz /lzj/
- [root@node01 ~]# hdfs dfs -D dfs.blocksize=1048576 -put zookeeper-3.4.5.tar.gz /lzj/

## CX | 关闭集群

- [root@node01 ~]# stop-dfs.sh

  - ```
    Stopping namenodes on [node01]
    Last login: Fri Oct 30 22:06:20 CST 2020 on pts/0
    node01: Warning: Permanently added 'node01,192.168.58.101' (ECDSA) to the list of known hosts.
    Stopping datanodes
    Last login: Fri Oct 30 22:16:34 CST 2020 on pts/0
    node03: Warning: Permanently added 'node03,192.168.58.103' (ECDSA) to the list of known hosts.
    node02: Warning: Permanently added 'node02,192.168.58.102' (ECDSA) to the list of known hosts.
    node01: Warning: Permanently added 'node01,192.168.58.101' (ECDSA) to the list of known hosts.
    Stopping secondary namenodes [node02]
    Last login: Fri Oct 30 22:16:35 CST 2020 on pts/0
    node02: Warning: Permanently added 'node02,192.168.58.102' (ECDSA) to the list of known hosts.
    ```

- 关机拍摄快照

  - [123]# shutdown -h now

# D MR&YARN

Hadoop搭建yarn环境

|        | NameNode01 | NameNode02 | DateNode | ZKFC | ZooKeeper | JournalNode | ResourceManager | NodeManager |
| ------ | ---------- | ---------- | -------- | ---- | --------- | ----------- | --------------- | ----------- |
| Node01 | *          |            | *        | *    | *         | *           | *               | *           |
| Node02 |            | *          | *        | *    | *         | *           |                 | *           |
| Node03 |            |            | *        |      | *         | *           | *               | *           |

* yarn环境搭建基于前面HA环境

## D1 | 切换工作目录

- [root@node01 ~]# cd /opt/lzj/hadoop-3.1.2/etc/hadoop/

## D2 | 修改集群环境

- [root@node01 hadoop]# vim hadoop-env.sh

  - ```sh
    ##继续添加用户信息
    export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64
    export HDFS_NAMENODE_USER=root
    export HDFS_DATANODE_USER=root
    export HDFS_ZKFC_USER=root
    export HDFS_JOURNALNODE_USER=root
    export YARN_RESOURCEMANAGER_USER=root
    export YARN_NODEMANAGER_USER=root
    ```

## D3 | 修改配置文件

- [root@node01 hadoop]# vim mapred-site.xml 

  - ```xml
    <!-- 指定mr框架为yarn方式 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!-- 指定mapreduce jobhistory地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node01:10020</value>
    </property>
    <!-- 任务历史服务器的web地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node01:19888</value>
    </property>
    <!-- 配置运行过的日志存放在hdfs上的存放路径 -->
    <property>
    	<name>mapreduce.jobhistory.done-dir</name>
    	<value>/history/done</value>
    </property>
    <!-- 配置正在运行中的日志在hdfs上的存放路径 -->
    <property>
    	<name>mapreudce.jobhistory.intermediate.done-dir</name>
    	<value>/history/done/done_intermediate</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>
        /opt/lzj/hadoop-3.1.2/etc/hadoop,
        /opt/lzj/hadoop-3.1.2/share/hadoop/common/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/common/lib/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/hdfs/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/hdfs/lib/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/mapreduce/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/mapreduce/lib/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/yarn/*,
        /opt/lzj/hadoop-3.1.2/share/hadoop/yarn/lib/*
        </value>
    </property>
    ```

- [root@node01 hadoop]# vim yarn-site.xml 

  - ```xml
    <!-- 开启RM高可用 -->
    <property>
    	<name>yarn.resourcemanager.ha.enabled</name>
    	<value>true</value>
    </property>
    <!-- 指定RM的cluster id -->
    <property>
    	<name>yarn.resourcemanager.cluster-id</name>
    	<value>yarn-lzj</value>
    </property>
    <!-- 指定RM的名字 -->
    <property>
    	<name>yarn.resourcemanager.ha.rm-ids</name>
    	<value>rm1,rm2</value>
    </property>
    <!-- 分别指定RM的地址 -->
    <property>
    	<name>yarn.resourcemanager.hostname.rm1</name>
    	<value>node01</value>
    </property>
    <property>
    	<name>yarn.resourcemanager.hostname.rm2</name>
    	<value>node03</value>
    </property>
    <property>
    	<name>yarn.resourcemanager.webapp.address.rm1</name>
    	<value>node01:8088</value>
    </property>
    <property>
    	<name>yarn.resourcemanager.webapp.address.rm2</name>
    	<value>node03:8088</value>
    </property>
    
    <!-- 指定zk集群地址 -->
    <property>
    	<name>yarn.resourcemanager.zk-address</name>
    	<value>node01:2181,node02:2181,node03:2181</value>
    </property>
    <property>
    	<name>yarn.nodemanager.aux-services</name>
    	<value>mapreduce_shuffle</value>
    </property>
    <!-- 开启日志聚合 -->
    <property>
    	<name>yarn.log-aggregation-enable</name>
    	<value>true</value>
    </property>
    <property>
    	<name>yarn.log-aggregation.retain-seconds</name>
    	<value>86400</value>
    </property>
    <!-- 启用自动恢复 -->
    <property>
    	<name>yarn.resourcemanager.recovery.enabled</name>
    	<value>true</value>
    </property>
    <!-- 制定resourcemanager的状态信息存储在zookeeper集群上 -->
    <property>
    	<name>yarn.resourcemanager.store.class</name>
    	<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    
    <!-- Whether virtual memory limits will be enforced for containers.  -->
    <property>
    	<name>yarn.nodemanager.vmem-check-enabled</name>
    	<value>false</value>
    </property>
    
    <property>
    	<name>yarn.nodemanager.vmem-pmem-ratio</name>
    	<value>3</value>
    </property>
    ```

## D4 | 拷贝到其他节点

- ```
  [root@node01 hadoop]# scp yarn-site.xml mapred-site.xml root@node02:`pwd`
  [root@node01 hadoop]# scp yarn-site.xml mapred-site.xml root@node03:`pwd`
  ```

## D5 | 开启集群

- [123]zkServer.sh start
- [root@node01 hadoop]# start-dfs.sh
- [root@node01 hadoop]# start-yarn.sh
- [root@node01 hadoop]# mr-jobhistory-daemon.sh start historyserver

## D6 | 关机拍摄快照

- [1]stop-all.sh

- [1]mr-jobhistory-daemon.sh stop historyserver

- [123]zkServer.sh stop

# E Hive

Hive的安装

| 节点\功能 | metastore | hiveserver2 | client |
| --------- | --------- | ----------- | ------ |
| node01    | *         | *           |        |
| node02    |           |             | *      |
| node03    |           |             | *      |

PS: 安装前 请确认当前集群已经安装了Mysql数据库和Hadoop的Ha+Yarn

## E1 | 下载上传解压

- 下载地址

  - http://archive.apache.org/dist/hive/

  - ```
    [root@node01 ~]# tar -zxvf apache-hive-3.1.2-bin.tar.gz 
    [root@node01 ~]# 
    [root@node01 ~]# cd /opt/bdp/apache-hive-3.1.2-bin/conf
    ```

## E2 | 修改配置文件

- 配置hive-env.sh

  - [root@node01 conf]# cp hive-env.sh.template hive-env.sh

  - [root@node01 conf]# vim hive-env.sh

  - ```shell
    HADOOP_HOME=/opt/bdp/hadoop-3.1.2/
    export HIVE_CONF_DIR=/opt/bdp/apache-hive-3.1.2-bin/conf 
    export HIVE_AUX_JARS_PATH=/opt/bdp/apache-hive-3.1.2-bin/lib
    ```

- 创建hive-site.xml

  - [root@node01 conf]# cp hive-default.xml.template hive-site.xml

  - [root@node01 conf]# vim hive-site.xml

  - 删除多余的配置文件  6897dd

  - ```xml
    <!-- 数据库相关配置 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <!-- 美化打印数据 -->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    
    <!-- hive server -->
    <property>
        <name>hive.server2.webui.host</name>
        <value>node01</value>
    </property>
    <property>
        <name>hive.server2.webui.port</name>
        <value>10002</value>
    </property>
    
    <!-- 数据存储位置 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/hive/warehouse</value>
    </property>
    
    ```

- 修改core-site.xml

  - [root@node01 conf]# vim /opt/bdp/hadoop-3.1.2/etc/hadoop/core-site.xml

  - 后面添加配置信息

  - ```xml
    <!--该参数表示可以通过httpfs接口hdfs的ip地址限制-->
    <property>
      <name>hadoop.proxyuser.root.hosts</name>
      <value>*</value>
    </property>
    <!--通过httpfs接口访问的用户获得的群组身份-->
    <property>
      <name>hadoop.proxyuser.root.groups</name>
      <value>*</value>
    </property>
    ```

## E3 | 配置日志组件

- 创建文件目录

  - [root@node01  ~]# mkdir -p /opt/bdp/apache-hive-3.1.2-bin/logs

  - [root@node01 conf]# cp hive-log4j2.properties.template  hive-log4j2.properties

  - [root@node01 conf]# vim hive-log4j2.properties

  - ```
    property.hive.log.dir = /opt/bdp/apache-hive-3.1.2-bin/logs
    ```

## E4 | 添加驱动包

- Mysql驱动添加到hive的lib目录下

- [root@node01 ~]# cp ~/mysql-connector-java-5.1.32-bin.jar /opt/bdp/apache-hive-3.1.2-bin/lib/

- Guava包

  - 首先要删除hadoop中的guava-*.jar包

  - ```
    [root@node01 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/guava-*.jar
    [root@node01 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/guava-*.jar
    ```

  - 将Hive的Guava拷贝给Hive

  - ```
    [root@node01 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/
    [root@node01 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/
    ```


## E5 | 配置环境变量

- vim /etc/profile

  - ```
    export HIVE_HOME=/opt/bdp/apache-hive-3.1.2-bin
    export PATH=$HIVE_HOME/bin:$PATH
    ```

- source /etc/profile

## E6 | 拷贝到其他节点

- hive文件夹

  - ```
    [root@node02 ~]# scp -r root@node01:/opt/bdp/apache-hive-3.1.2-bin /opt/bdp/
    [root@node03 ~]# scp -r root@node01:/opt/bdp/apache-hive-3.1.2-bin /opt/bdp/
    ```

- 环境变量

  - ```
    [root@node01 ~]# scp /etc/profile root@node02:/etc/profile
    [root@node01 ~]# scp /etc/profile root@node03:/etc/profile
    ```

  - 【123】source /etc/profile

- core-stie.xml

  - ```
    [root@node01 ~]# scp /opt/bdp/hadoop-3.1.2/etc/hadoop/core-site.xml root@node02:/opt/bdp/hadoop-3.1.2/etc/hadoop/
    [root@node01 ~]# scp /opt/bdp/hadoop-3.1.2/etc/hadoop/core-site.xml root@node03:/opt/bdp/hadoop-3.1.2/etc/hadoop/
    ```

* jar包

  * ```
    [root@node02 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/guava-*.jar
    [root@node02 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/guava-*.jar
    [root@node03 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/guava-*.jar
    [root@node03 ~]# rm -rf /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/guava-*.jar
    
    [root@node02 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/
    [root@node02 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/
    
    [root@node03 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/common/lib/
    [root@node03 ~]# cp /opt/bdp/apache-hive-3.1.2-bin/lib/guava-*.jar /opt/bdp/hadoop-3.1.2/share/hadoop/hdfs/lib/
    ```

## E7 | 客户端配置文件

- 选取node03为客户端节点

  - [23] vim /opt/bdp/apache-hive-3.1.2-bin/conf/hive-site.xml

  - ```xml
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/hive/warehouse</value>
    </property>
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value>
    </property>
    <!--指定hive.metastore.uris的port.为了启动metastore服务的时候不用指定端口-->
    <!--hive ==service metastore -p 9083 & | hive ==service metastore-->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://node01:9083</value>
    </property>
    ```

## E8 | 启动集群

- 启动Zookeeper
  - 【123】zkServer.sh start
- 启动Hdfs+Yarn
  - [root@node01 ~]# start-all.sh
- 初始化数据库
  - [root@node01 ~]# schematool -dbType mysql -initSchema
  - ![image-20201111202011359](001_%E6%89%80%E6%9C%89%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.assets/image-20201111202011359.png)
  - ![image-20201111202030606](001_%E6%89%80%E6%9C%89%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.assets/image-20201111202030606.png)
- 启动Hive
  - [root@node01 ~]# hive --service metastore
  - [root@node01 ~]# nohup hive --service metastore > /dev/null 2>&1 &
  - [root@node03 ~]# hive
- 启动HiveServer2
  - [root@node01 ~]# hiveserver2
  - [root@node01 ~]# nohup hiveserver2 > /dev/null 2>&1 &
  - [root@node03 ~]# beeline -u jdbc:hive2://node01:10000 -n root