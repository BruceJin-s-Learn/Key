练习代码

# A MR

## MapReduce案例

### WordCount项目

#### Java实现

- pom.xml

  - ```xml
    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <!-- Hadoop版本控制 -->
        <hadoop.version>3.1.2</hadoop.version>
        <!-- commons-io版本控制 -->
        <commons-io.version>2.4</commons-io.version>
    </properties>
    
    <dependencies>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/commons-io/commons-io -->
        <dependency>
            <groupId>commons-io</groupId>
            <artifactId>commons-io</artifactId>
            <version>${commons-io.version}</version>
        </dependency>
        <dependency>
            <groupId>com.janeluo</groupId>
            <artifactId>ikanalyzer</artifactId>
            <version>2012_u6</version>
        </dependency>
    </dependencies>
    ```

- Job代码

  - ```java
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    
    public class WordCountJob {
        public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
            //获取配置文件
            Configuration configuration = new Configuration(true);
            //本地模式运行
            configuration.set("mapreduce.framework.name", "local");
            //创建任务
            Job job = Job.getInstance(configuration);
            //设置任务主类
            job.setJarByClass(WordCountJob.class);
            //设置任务
            job.setJobName("lzj-wordcount-" + System.currentTimeMillis());
            //设置Reduce的数量
            job.setNumReduceTasks(2);
    
            //设置数据的输入路径
            FileInputFormat.setInputPaths(job, new Path("/lzj/harry.txt"));
            //设置数据的输出路径
            FileOutputFormat.setOutputPath(job, new Path("/lzj/result/wordcount_" + System.currentTimeMillis()));
    
            //设置Map的输入的key和value类型
            job.setMapOutputKeyClass(Text.class);
            job.setMapOutputValueClass(IntWritable.class);
            //设置Map和Reduce的处理类
            job.setMapperClass(WordCountMapper.class);
            job.setReducerClass(WordCountReducer.class);
            //提交任务
            job.waitForCompletion(true);
        }
    }
    ```

- Job的Mapper代码

  - ```java
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;
    
    import java.io.IOException;
    
    public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    
        //创建对象
        private IntWritable one = new IntWritable(1);
    
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            //替换特殊字符
            String valueString = value.toString();
            valueString = valueString.replaceAll("[^a-zA-Z0-9'\\s]", "");
            //切分字符串
            String[] values = valueString.split(" ");
            //向里面添加数据
            for (String val : values) {
                context.write(new Text(val), one);
            }
        }
    }
    ```

- Job的Reducer代码

  - ```java
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;
    
    import java.io.IOException;
    import java.util.Iterator;
    
    public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    
        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            //获取迭代器
            Iterator<IntWritable> iterator = values.iterator();
            //声明一个计数器
            int count = 0;
            while (iterator.hasNext()) {
                count += iterator.next().get();
            }
            //输出数据
            context.write(key, new IntWritable(count));
        }
    }
    
    ```

#### MR执行的方式

- Linux端执行方式
  - hadoop jar wordcount.jar com.lzj.wordcount.WordCountJob

- window端本地化执行
  - 拷贝Hadoop配置文件
  - configuration.set("mapreduce.framework.name", "local");

## MapReduce源码分析

### Split

- 快捷键

  - ```
    ctrl + alt + 方向键 ：查看上一个或者下一个方法
    ctrl + shift + alt + C 　拷贝方法的全名
    ctrl + alt + b 查看当前接口的实现类
    ```

- 源代码的分析从提交任务开始

  - ```java
    job.waitForCompletion(true);
    ```

- org.apache.hadoop.mapreduce.Job#waitForCompletion

  - ```java
    //Submit the job to the cluster and wait for it to finish.
    //判断当前的状态
    if (state == JobState.DEFINE) {
    	//------------------------------------------------------------------------------关键代码
        submit();
    }
    //监控任务的运行状态
    if (verbose) {
        //Monitor a job and print status in real-time as progress is made and tasks fail.
        monitorAndPrintJob();
    }
    //返回任务状态
    return isSuccessful();
    ```

- org.apache.hadoop.mapreduce.Job#submit

  - ```java
    //确认当前任务的状态
    ensureState(JobState.DEFINE);
    //mapreduce1.x和2.x,但是2的时候将1的好多方法进行了优化
    setUseNewAPI();
    //获取当前任务所运行的集群
    connect();
    //Provides a way to access information about the map/reduce cluster.
    cluster =  new Cluster(getConfiguration());
    //创建Job的提交器
    final JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(), cluster.getClient());
    //提交任务到系统去执行 
    //------------------------------------------------------------------------------关键代码
    //Internal method for submitting jobs to the system
    status = submitter.submitJobInternal(Job.this, cluster)
    //任务的状态修改为运行
    state = JobState.RUNNING;
    ```

- org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal

  - ```java
    //验证job输出
    checkSpecs(job);
    //生成并设置新的JobId
    JobID jobId = submitClient.getNewJobID();
    job.setJobID(jobId);
    //获取任务的提交目录
    Path submitJobDir = new Path(jobStagingArea, jobId.toString());
    //------------------------------------------------------------------------------关键代码
    // Create the splits for the job  197行
    int maps = writeSplits(job, submitJobDir);
    //设置map的数量，其中map的数量就等于切片的数量
    conf.setInt(MRJobConfig.NUM_MAPS, maps);
    ```

- org.apache.hadoop.mapreduce.JobSubmitter#writeSplits

  - ```java
    //------------------------------------------------------------------------------关键代码
    //使用新API
    maps = writeNewSplits(job, jobSubmitDir);
    ```

- org.apache.hadoop.mapreduce.JobSubmitter#writeNewSplits

  - ```java
    //获取配置文件
    Configuration conf = job.getConfiguration();
    //------------------------------------------------------------------------------关键代码InputFormat
    //获取文件读取器  org.apache.hadoop.mapreduce.lib.input.TextInputFormat
    InputFormat<?, ?> input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);
    
    //------------------------------------------------------------------------------关键代码getSplits
    List<InputSplit> splits = input.getSplits(job);
    //将List转成数组
    T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);
    
    // sort the splits into order based on size, so that the biggest
    Arrays.sort(array, new SplitComparator());
    //任务创建切片文件
    JobSplitWriter.createSplitFiles(jobSubmitDir, conf,jobSubmitDir.getFileSystem(conf), array);
    //返回切片的数目
    return array.length;
    ```

- org.apache.hadoop.mapreduce.task.JobContextImpl#getInputFormatClass

  - ```java
    //返回创建的TextInputFormat对象
    return (Class<? extends InputFormat<?,?>>) conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);
    
    //getClass的操作是如果有值返回值，没有的话使用默认值
    getClass(String name, Class<?> defaultValue)
    ```

- org.apache.hadoop.mapreduce.lib.input.FileInputFormat#getSplits

  - public class TextInputFormat extends FileInputFormat<LongWritable, Text>

  - ```java
    //Generate the list of files and make them into FileSplits.
    //Math.max(1,1)
    //getFormatMinSplitSize()一个切片最少应该拥有1个字节
    //getMinSplitSize(job) 读取程序员设置的切片的最小值，如果没有设置默认读取1
    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
    //读取程序员设置的切片的最大值，如果没有设置默认读取Long.MAX_VALUE
    long maxSize = getMaxSplitSize(job);
    
    //创建一个List存放切片
    List<InputSplit> splits = new ArrayList<InputSplit>();
    //获取要分析的文件列表
    List<FileStatus> files = listStatus(job);
    
    //开始遍历要分析文件的路径
    for (FileStatus file : files) {
        //获取文件路径
        Path path = file.getPath();
        //获取文件的长度，文件拥有的字节数
        long length = file.getLen();
        //如果文件长度不为0
        if (length != 0) {
            //获取文件对应的Blocks信息
            BlockLocation[] blkLocations;
            if (file instanceof LocatedFileStatus) {
                blkLocations = ((LocatedFileStatus) file).getBlockLocations();
            } else {
                FileSystem fs = path.getFileSystem(job.getConfiguration());
                blkLocations = fs.getFileBlockLocations(file, 0, length);
            }
            //判断文件是否可以进行切片
            if (isSplitable(job, path)) {
                //获取Block的大小
                long blockSize = file.getBlockSize();
                //切片的默认大小为 128M
                //blockSize  128M, minSize 1byte, maxSize  long.Max_ValueBytes
                //return Math.max(minSize, Math.min(maxSize, blockSize));
                //minSize 64M  ----> 128M
                //minSize 256M ----> 256M
                //maxSize 64M ----> 64M
                //maxSize 256M ---->128M
                long splitSize = computeSplitSize(blockSize, minSize, maxSize);
    			
                //声明一个变量存放字节的余额  256M
                long bytesRemaining = length;
                //查看剩余的容量是否能达到阈值  SPLIT_SLOP 
                //private static final double SPLIT_SLOP = 1.1
                while (((double) bytesRemaining) / splitSize > SPLIT_SLOP) {
                    int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
                    //这个方法工厂专门用来创建切片
                    //切片生成之后添加到List
                    //org.apache.hadoop.mapreduce.lib.input.FileInputFormat#makeSplit
                    splits.add(makeSplit(path, length - bytesRemaining, splitSize,
                                         blkLocations[blkIndex].getHosts(),
                                         blkLocations[blkIndex].getCachedHosts()));
                    //每次创建切片后，将使用的部分删除
                    bytesRemaining -= splitSize;
                }
    			//判断剩余的容量是否为0
                //最后一个切片的数据范围是(0,1 , 1.1]
                if (bytesRemaining != 0) {
                    int blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);
                    splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,
                                         blkLocations[blkIndex].getHosts(),
                                         blkLocations[blkIndex].getCachedHosts()));
                }
            } else { // not splitable
                //如果发现文件不能切片，将整个文件作为一个切片
                splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                                     blkLocations[0].getCachedHosts()));
            }
        } else {
            //Create empty hosts array for zero length files
            splits.add(makeSplit(path, 0, length, new String[0]));
        }
    }
    // Save the number of input files for metrics/loadgen
    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
    //返回切片的容器
    return splits;
    ```

### MapTask

- org.apache.hadoop.mapred.MapTask#run

  - ```java
    //使用新的API
    boolean useNewApi = job.getUseNewMapper();
    //------------------------------------------------------------------------------关键代码initialize
    //初始化MapTask
    initialize(job, getJobID(), reporter, useNewApi);
    //------------------------------------------------------------------------------关键代码runNewMapper
    //开始运行Task
    runNewMapper(job, splitMetaInfo, umbilical, reporter);
    ```

- org.apache.hadoop.mapred.Task#initialize

  - ```java
    //JOB的上下文参数
    jobContext = new JobContextImpl(job, id, reporter);
    //Map的上下文参数
    taskContext = new TaskAttemptContextImpl(job, taskId, reporter);
    //创建Map数据的写出器
    outputFormat = ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job);
    		//真正的写出对象
    		org.apache.hadoop.mapreduce.task.JobContextImpl#getOutputFormatClass
                return (Class<? extends OutputFormat<?,?>>) 
                        conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);
    //创建Map任务的提交器
    committer = outputFormat.getOutputCommitter(taskContext);
    		//真正的提交器对象
    		org.apache.hadoop.mapreduce.lib.output.FileOutputFormat#getOutputCommitter
                committer = new FileOutputCommitter(output, context);
    //获取写出的路径
    Path outputPath = FileOutputFormat.getOutputPath(conf);
    ```

- org.apache.hadoop.mapred.MapTask#runNewMapper

  - ```java
    // make a task context so we can get the classes
    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
    	new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, getTaskID(),reporter);
    // make a mapper--com.lzj.wordcount.WordCountMapper
    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
        (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);
    // make the input format--org.apache.hadoop.mapreduce.lib.input.TextInputFormat
    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =
        (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)
        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);
    // rebuild the input split
    org.apache.hadoop.mapreduce.InputSplit split = null;
    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),splitIndex.getStartOffset());
    // 创建记录读取器
    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =
          	new NewTrackingRecordReader<INKEY,INVALUE>(split, inputFormat, reporter, taskContext);
    		//创建真正的读取器
    		//org.apache.hadoop.mapred.MapTask.NewTrackingRecordReader#NewTrackingRecordReader
    		this.real = inputFormat.createRecordReader(split, taskContext);
    			//使用inputFormat创建读取器
    			//org.apache.hadoop.mapreduce.lib.input.TextInputFormat#createRecordReader
    			return new LineRecordReader(recordDelimiterBytes);
    
    // 创建记录写出器
    org.apache.hadoop.mapreduce.RecordWriter output = null;
    output = new NewOutputCollector(taskContext, job, umbilical, reporter);
    // 创建Map的上下文对象
    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> 
        mapContext = 
          new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), 
              input, output, 
              committer, 
              reporter, split);
    // 创建mapContext的包装类
    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context 
            mapperContext = 
              new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(
                  mapContext);
    // 初始化切片信息
    input.initialize(split, mapperContext);
    //开始执行Mapper方法，就是自己的Mapper实现类
    mapper.run(mapperContext);
    mapPhase.complete();
    setPhase(TaskStatus.Phase.SORT);
    statusUpdate(umbilical);
    //关闭输入
    input.close();
    input = null;
    //关闭输出（将缓冲区最后的数据写出，并合并这些文件）
    output.close(mapperContext);
    output = null;
    
    ```

- org.apache.hadoop.mapred.MapTask.NewTrackingRecordReader#initialize

  - ```java
    //LineRecordReader执行初始化
    real.initialize(split, context);
    ```

- org.apache.hadoop.mapreduce.lib.input.LineRecordReader#initialize

  - ```java
    //获取切片
    FileSplit split = (FileSplit) genericSplit;
    //配置信息
    Configuration job = context.getConfiguration();
    //一行最多读取的数据量
    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);
    //获取切片的开始和结束偏移量
    start = split.getStart();
    end = start + split.getLength();
    //获取文件路径
    final Path file = split.getPath();
    
    // open the file and seek to the start of the split
    final FileSystem fs = file.getFileSystem(job);
    fileIn = fs.open(file);
    
    //将读取器定位到切片的开始位置
    fileIn.seek(start);
    //创建输入流
    in = new UncompressedSplitLineReader(fileIn, job, this.recordDelimiterBytes, split.getLength());
    filePosition = fileIn;
    // If this is not the first split, we always throw away first record
    // because we always (except the last split) read one extra line in
    // next() method.
    if (start != 0) {
        start += in.readLine(new Text(), 0, maxBytesToConsume(start));
    }
    this.pos = start;
    ```

- org.apache.hadoop.mapreduce.Mapper#run

  - ```java
    //初始化
    setup(context);
    try {
        //1判断是否为最后一行2为key设置值3为value设置值
    	while (context.nextKeyValue()) {
            //三个参数分别为：key value
    		map(context.getCurrentKey(), context.getCurrentValue(), context);
    	}
    } finally {
        //清空操作
    	cleanup(context);
    }
    ```

- org.apache.hadoop.mapreduce.lib.input.LineRecordReader#nextKeyValue

  - ```java
    //偏移量
    key = new LongWritable();
    //设置本次读取的开始位置
    key.set(pos);
    //一行数据
    value = new Text();
    //We always read one extra line 读取一行的数据
    if (pos == 0) {
        newSize = skipUtfByteOrderMark();
    } else {
        newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));
        //下次读取数据的位置
        pos += newSize;
    }
    ```

- org.apache.hadoop.mapreduce.lib.input.LineRecordReader#skipUtfByteOrderMark

  - ```java
    //每次空读一行数据，绕过第一行代码
    int newSize = in.readLine(value, newMaxLineLength, maxBytesToConsume(pos));
    pos += newSize;
    ```

    

### KvBuffer

- org.apache.hadoop.mapred.MapTask.NewOutputCollector#NewOutputCollector

  - ```java
    //创建收集器
    collector = createSortingCollector(job, reporter);
    //获取reduce的数量
    partitions = jobContext.getNumReduceTasks();
    if (partitions > 1) {
        partitioner = (org.apache.hadoop.mapreduce.Partitioner<K, V>)
            	ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);
    } else {
        partitioner = new org.apache.hadoop.mapreduce.Partitioner<K, V>() {
            @Override
            public int getPartition(K key, V value, int numPartitions) {
                return partitions - 1;
            }
        };
    }
    ```

- org.apache.hadoop.mapred.MapTask#createSortingCollector

  - ```java
    //获取上下文对象
    MapOutputCollector.Context context = new MapOutputCollector.Context(this, job, reporter);
    //获取收集器的Class
    Class<?>[] collectorClasses = job.getClasses(
          JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);
    //获取MapOutputCollector的自雷
    Class<? extends MapOutputCollector> subclazz = clazz.asSubclass(MapOutputCollector.class);
    //通过反射创建一个收集器--org.apache.hadoop.mapred.MapTask.MapOutputBuffer
    MapOutputCollector<KEY, VALUE> collector = ReflectionUtils.newInstance(subclazz, job);
    //执行初始化操作
    collector.init(context);
    //最终将创建的写出器返回
    return collector;
    ```

- org.apache.hadoop.mapred.MapTask.MapOutputBuffer#init

  - ```java
    //获取溢写的阈值
    final float spillper = job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);
    //缓冲区数据的大小100M
    final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);
    //数据的大小 1024*1024
    indexCacheMemoryLimit = job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,
                                   INDEX_CACHE_MEMORY_LIMIT_DEFAULT)
    //获取排序器--快速排序
    sorter = ReflectionUtils.newInstance(job.getClass("map.sort.class",
                 QuickSort.class, IndexedSorter.class), job);
    //设置容量 100M
    int maxMemUsage = sortmb << 20;
    //结果肯定是16的整数倍
    maxMemUsage -= maxMemUsage % METASIZE;
    //缓冲区
    kvbuffer = new byte[maxMemUsage];
    //kvbuffer开始进行初始化
    bufvoid = kvbuffer.length;
    kvmeta = ByteBuffer.wrap(kvbuffer).order(ByteOrder.nativeOrder()).asIntBuffer();
    setEquator(0);
    bufstart = bufend = bufindex = equator;
    kvstart = kvend = kvindex;
    
    maxRec = kvmeta.capacity() / NMETA;
    softLimit = (int)(kvbuffer.length * spillper);
    bufferRemaining = softLimit;
    
    //获取比较器
    comparator = job.getOutputKeyComparator();
    //获取Map的key和value输出类型
    keyClass = (Class<K>)job.getMapOutputKeyClass();
    valClass = (Class<V>)job.getMapOutputValueClass();
    //序列化Key和Value
    keySerializer = serializationFactory.getSerializer(keyClass);
    keySerializer.open(bb);
    valSerializer = serializationFactory.getSerializer(valClass);
    valSerializer.open(bb);
    //创建溢写线程，并让溢写线程处于等待，当达到阈值的时候开始溢写
    spillInProgress = false;
    minSpillsForCombine = job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);
    spillThread.setDaemon(true);
    spillThread.setName("SpillThread");
    spillLock.lock();
    try {
        spillThread.start();
        while (!spillThreadRunning) {
            spillDone.await();
        }
    } 
    ```

- org.apache.hadoop.mapred.JobConf#getOutputKeyComparator

  - ```java
    //获取比较器
    Class<? extends RawComparator> ts = getClass(JobContext.KEY_COMPARATOR, null, RawComparator.class);
    //如果自定义了比较器，创建自定义比较器对象
    if (ts != null)
        return ReflectionUtils.newInstance(ts, this);
    //如果没有创建比较器
    return WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), this);
    	//默认的比较器对象--org.apache.hadoop.io.WritableComparator
    	comparator = new WritableComparator(c, conf, true);
    ```

- org.apache.hadoop.mapreduce.task.JobContextImpl#getPartitionerClass

  - ```java
    //创建分区器
    return (Class<? extends Partitioner<?,?>>) 
             conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);
            //分区器具体执行的代码
            //org.apache.hadoop.mapreduce.lib.partition.HashPartitioner#getPartition
    		return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
    ```

### Spill

- org.apache.hadoop.mapred.MapTask.NewOutputCollector#write

  - ```java
    //开始收集数据
    collector.collect(key, value, partitioner.getPartition(key, value, partitions));
    ```

- org.apache.hadoop.mapred.MapTask.MapOutputBuffer#collect

  - ```java
    //元数据存储区
    bufferRemaining -= METASIZE;
    //判断是否需要进行溢写，如果需要进行准备工作
    //如果需要溢写唤醒SpillThread线程，调用run方法，开始SortAndSpill
    	org.apache.hadoop.mapred.MapTask.MapOutputBuffer#sortAndSpill	
    //如果不满足将数据存储到KvBuffer
    ```

### Merge

- org.apache.hadoop.mapred.MapTask.MapOutputBuffer#flush

  - ```java
    //将缓冲区中不满80%的数据也写出到硬盘
    sortAndSpill();
    //合并曾经溢写出的数据块
    mergeParts();
    //当前Map准备好进入到下一个阶段
    sortPhase.startNextPhase();
    ```

### ReduceTask

- org.apache.hadoop.mapred.ReduceTask#run

  - ```java
    //进行初始化操作
    initialize(job, getJobID(), reporter, useNewApi);
    //获取Key和Value的迭代器
    RawKeyValueIterator rIter = null;
    
    //创建一个
    Class combinerClass = conf.getCombinerClass();
    CombineOutputCollector combineCollector =
       		(null != combinerClass) ?
                          new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;
    //创建一个Shuffer
    Class<? extends ShuffleConsumerPlugin> clazz =
              job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class,ShuffleConsumerPlugin.class);
    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);
    
    //创建一个上下文对象，并且对Shuffer进行初始化
    ShuffleConsumerPlugin.Context shuffleContext =
              new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical,
                            super.lDirAlloc, reporter, codec,
                            combinerClass, combineCollector,
                            spilledRecordsCounter, reduceCombineInputCounter,
                            shuffledMapsCounter,
                            reduceShuffleBytes, failedShuffleCounter,
                            mergedMapOutputsCounter,
                            taskStatus, copyPhase, sortPhase, this,
                            mapOutputFile, localMapFiles);
    //已经初始化了合并器
    shuffleConsumerPlugin.init(shuffleContext);
    //执行Shuffer，并且返回key value的迭代器---MergeQueue
    rIter = shuffleConsumerPlugin.run();
    //获取Key的输出类型
    Class keyClass = job.getMapOutputKeyClass();
    Class valueClass = job.getMapOutputValueClass();
    //获取分组比较器（reduce阶段优先使用分组比较器，如果没有设置就使用原来的比较器）
    RawComparator comparator = job.getOutputValueGroupingComparator();
    //开始执行Reduce任务
    runNewReducer(job, umbilical, reporter, rIter, comparator,keyClass, valueClass);
    ```

- org.apache.hadoop.mapred.Task#initialize

  - ```java
    //获取Job和Reduce的Context
    jobContext = new JobContextImpl(job, id, reporter);
    taskContext = new TaskAttemptContextImpl(job, taskId, reporter);
    //返回数据的写出对象----org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
    outputFormat = ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job);
    //创建提交对象----org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
    committer = outputFormat.getOutputCommitter(taskContext);
    //获取数据输出的路径
    Path outputPath = FileOutputFormat.getOutputPath(conf);
    ```

- org.apache.hadoop.mapreduce.task.reduce.Shuffle#run

  - ```java
    // Start the map-completion events fetcher thread
    final EventFetcher<K, V> eventFetcher =
        new EventFetcher<K, V>(reduceId, umbilical, scheduler, this,maxEventsToFetch);
    eventFetcher.start();
    //判断map和reduce是否在一个节点
    boolean isLocal = localMapFiles != null;
    //开启拉取的线程数，本地为1，其他节点为5
    final int numFetchers = isLocal ? 1 :jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);
    Fetcher<K, V>[] fetchers = new Fetcher[numFetchers];
    //开始去拉取数据
    fetchers[0].start();
    //关闭拉取事件
    eventFetcher.shutDown();
    // Stop the map-output fetcher threads
    for (Fetcher<K, V> fetcher : fetchers) {
        fetcher.shutDown();
    }
    //开始获取KeyValue的迭代器
    RawKeyValueIterator kvIter = merger.close();
    
    return kvIter;
    ```

- org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#close

  - ```java
    //返回最后一次合并的数据的迭代器
    return finalMerge(jobConf, rfs, memory, disk);
    ```

- org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#finalMerge

  - ```java
    //获取map输出数据的类型
    Class<K> keyClass = (Class<K>)job.getMapOutputKeyClass();
    Class<V> valueClass = (Class<V>)job.getMapOutputValueClass();
    //获取一个比较器 ----org.apache.hadoop.io.WritableComparator
    final RawComparator<K> comparator = (RawComparator<K>)job.getOutputKeyComparator();
    //返回key迭代器----org.apache.hadoop.mapred.Merger.MergeQueue
    final RawKeyValueIterator rIter = Merger.merge(job, fs,
                    keyClass, valueClass, memDiskSegments, numMemDiskSegments,
                    tmpDir, comparator, reporter, spilledRecordsCounter, null,
                mergePhase);
    
    ```

- org.apache.hadoop.mapred.ReduceTask#runNewReducer

  - ```java
    //获取迭代器
    final RawKeyValueIterator rawIter = rIter;
    //使用匿名内部类创建一个新的对象
    rIter = new RawKeyValueIterator() {
        public void close() throws IOException {
            rawIter.close();
        }
    
        public DataInputBuffer getKey() throws IOException {
            return rawIter.getKey();
        }
    
        public Progress getProgress() {
            return rawIter.getProgress();
        }
    
        public DataInputBuffer getValue() throws IOException {
            return rawIter.getValue();
        }
    
        public boolean next() throws IOException {
            boolean ret = rawIter.next();
            reporter.setProgress(rawIter.getProgress().getProgress());
            return ret;
        }
    };
    //本次任务的上下文对象
    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
               new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,getTaskID(), reporter);
    //本次要执行的Reducer --com.lzj.WordCountReducer
    org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE> reducer =
                    (org.apache.hadoop.mapreduce.Reducer<INKEY, INVALUE, OUTKEY, OUTVALUE>)
                            ReflectionUtils.newInstance(taskContext.getReducerClass(), job);
    //数据的写出器----org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.LineRecordWriter
    org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> trackedRW =
                    new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);
    //创建Reduce的上下文对象
     org.apache.hadoop.mapreduce.Reducer.Context
                    reducerContext = createReduceContext(reducer, job, getTaskID(),
                    rIter, reduceInputKeyCounter,
                    reduceInputValueCounter,
                    trackedRW,
                    committer,
                    reporter, comparator, keyClass,
                    valueClass);
    //开始执行reduce任务
    reducer.run(reducerContext);
    ```

- org.apache.hadoop.mapreduce.Reducer#run

  - ```java
    //判断是否还有数据可以读取，相同的key只会执行一次（hello hello hello hi hi hi 2次）
    while (context.nextKey()) {
        //context.getValues()-->private ValueIterable iterable = new ValueIterable();
        //values.iterator -->private ValueIterator iterator = new ValueIterator();
        //iterator.hasNext -->return firstValue || nextKeyIsSame;
        //iterator.next-->(firstValue?value:nextKeyValue())
        //
        reduce(context.getCurrentKey(), context.getValues(), context);
    }
    ```

- org.apache.hadoop.mapreduce.task.ReduceContextImpl#nextKey

  - ```java
    //hashMore 判断是否还有数据可以读取
    //是否还有数据
    if (hasMore) {
        //开始读取下一行
        return nextKeyValue();
    } else {
        //所有数据处理完成，reduce结束
        return false;
    }
    ```

- org.apache.hadoop.mapreduce.task.ReduceContextImpl#nextKeyValue

  - ```java
    //判断是否为key的第一个值
    firstValue = !nextKeyIsSame;
    //获取Key
    key = keyDeserializer.deserialize(key);
    value = valueDeserializer.deserialize(value);
    //获取序列化时Key和value的长度
    currentKeyLength = nextKey.getLength() - nextKey.getPosition();
    currentValueLength = nextVal.getLength() - nextVal.getPosition();
    //将数据写入到备份存储
    if (isMarked) {
        backupStore.write(nextKey, nextVal);
    }
    //判断下一次是否可以继续读取数据
    hasMore = input.next();
    //如果后面还有数据，我要判断nextKeyIsSame
    if (hasMore) {
        //获取下个Key
        nextKey = input.getKey();
        //首先是组比较器，否则就是默认的比较器
        nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0,
                                           currentRawKey.getLength(),
                                           nextKey.getData(),
                                           nextKey.getPosition(),
                                           nextKey.getLength() - nextKey.getPosition()
                                          ) == 0;
    } else {
        //如果读取不到数据，也就没有下一个了
        nextKeyIsSame = false;
    }
    ```

- org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.LineRecordWriter#write

  - ```java
    //以行的方式将数据写出
    out.write(newline);
    ```

## MapReduce案例

### 天气信息

- 每个地区，每天的最高温度和最低温度分别是多少？
- 每个地区，每个月最高的三个温度以及它对应的是几号

- ![image-20201106001412832](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/image-20201106001412832.png)


### 好友推荐系统

- 数据量
  - QQ好友推荐-->  
    - 每个QQ200好友
    - 5亿QQ号
- 解决思路
  - 需要按照行进行计算
  - 将相同推荐设置成相同的key，便于reduce统一处理
- 数据

```
tom hello hadoop cat
world hello hadoop hive
cat tom hive
mr hive hello
hive cat hadoop world hello mr
hadoop tom hive world hello
hello tom world hive mr
```

![image-20201105235503297](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/image-20201105235503297.png)



### PageRank

- PageRank是Sergey Brin与Larry Page于1998年在WWW7会议上提出来的，用来解决链接分析中网页排名的问题

  - PageRank是Google提出的算法，用于衡量特定网页相对于搜索引擎索引中的其他网页而言的重要程度
  - PageRank实现了将链接价值概念作为排名因素
  - 以后搜索对应关键词的时候，按照网站的权重顺序显示网页
  - ![1593363908557](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593363908557.png)

- 方法的原理

  - 投票算法

  - 作为网络搜索引擎，他们会将全网的网站全部爬取到自己的服务器进行分析

  - 然后分析出当前页面中到其他网站的外链（出链）

  - 同时也有其他的网站链接到当前网站（入链）

    - 入链的数量

      - 如果一个页面节点接收到的其他网页指向的入链数量越多，那么这个页面越重要

    - # 入链的质量

      - 质量高的页面会通过链接向其他页面传递更多的权重

    - ![1593364545338](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593364545338.png)

    - ![1593364555259](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593364555259.png)

    - 

- 算法过程

  - 首先每个网站默认的权重是一样的（十分制，百分制）
  - 然后将网站的权重平分给当前网站的所有出链(  10 / 5  =  2 )
  - 如果一个网站有多个入链，就将本次所有的入链得分累加到一起（2+4+7+1+10=24分）
  - 那么本次的得分会用于计算下次出链的计算 (24/5 = 4.8)
  - 重复迭代上面的过程，慢慢达到一个收敛值
  - 收敛标准是衡量本次计算精度的有效方法
    - 超过99.9%的网站pr值和上次一样
    - 所有的pr差值（本次和上次）累加求平均值不超过 0.0001
  - 停止运算

- 阻尼系数

  - 我自己申请100个域名，然后就指向自己的目标网站 www.lzj.com
    - 闭环
    - 只进不出
    - 只出不进
  - 修正PageRank计算公式：增加阻尼系数（damping factor）
    - d=0.85
  - 新的PR公式
    - ![1593364249854](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593364249854.png)
    - d：阻尼系数
    - M(i)：指向i的页面集合
    - L(j)：页面的出链数
    - PR(pj)：j页面的PR值
    - n：所有页面数

- 算法缺点

  - 第一，没有区分站内导航链接。很多网站的首页都有很多对站内其他页面的链接，称为站内导航链接。这些链接与不同网站之间的链接相比，肯定是后者更能体现PageRank值的传递关系。
  - 第二，没有过滤广告链接和功能链接。这些链接通常没有什么实际价值，前者链接到广告页面，后者常常链接到某个社交网站首页。
  - 第三，对新网页不友好。一个新网页的一般入链相对较少，即使它的内容的质量很高，要成为一个高PR值的页面仍需要很长时间的推广。
  - 针对PageRank算法的缺点，有人提出了**TrustRank算法**。其最初来自于2004年斯坦福大学和雅虎的一项联合研究，用来检测垃圾网站。TrustRank算法的工作原理：先人工去识别高质量的页面(即“种子”页面)，那么由“种子”页面指向的页面也可能是高质量页面，即其TR值也高，与“种子”页面的链接越远，页面的TR值越低。“种子”页面可选出链数较多的网页，也可选PR值较高的网站
  - TrustRank算法给出每个网页的TR值。将PR值与TR值结合起来，可以更准确地判断网页的重要性。

- 数据

  - ![1593364134263](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593364134263.png)

  - ```
    a b d
    b c
    c a b
    d b c
    
    a 1.0 b d
    b 1.0 c
    c 1.0 a b
    d 1.0 b c
    
    a 0.5 b d
    b 1.5 c
    c 1.5 a b
    d 0.5 b c
    
    a 0. b d
    b 1.25 c
    c 1.75 a b
    d 0.25 b c
    ```

- 具体实现

  - ```java
    package com.lzj.pagerank;
    
    import org.apache.commons.lang.StringUtils;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    import java.util.Arrays;
    
    public class PageRankJob {
    
        public static enum Mycounter {
            my
        }
    
        public static void main(String[] args) {
            //获取配置文件
            Configuration conf = new Configuration(true);
            //跨平台执行
            conf.set("mapreduce.app-submission.corss-paltform", "true");
            //如果分布式运行,必须打jar包 且,client在集群外非hadoop jar 这种方式启动,client中必须配置jar的位置
            //这个配置,只属于切换分布式到本地单进程模拟运行的配置
            //这种方式不是分布式,所以不用打jar包
            conf.set("mapreduce.framework.name", "local");
            //收敛的指标
            double convergence = 0.0001;
    
            //计算迭代收敛的次数
            int runCount = 0;
            //开始执行迭代收敛
            while (true) {
                //计数累加
                runCount++;
                try {
                    //向配置文件中设置一个变量
                    conf.setInt("runCount", runCount);
                    //获取分布式文件系统
                    FileSystem fs = FileSystem.get(conf);
                    //创建JOB,并设置基本信息
                    Job job = Job.getInstance(conf);
                    job.setJarByClass(PageRankJob.class);
                    job.setJobName("pagerank-" + runCount);
                    //设置Mapper的输出类型
                    job.setMapOutputKeyClass(Text.class);
                    job.setMapOutputValueClass(Text.class);
                    //设置Mapper和Reducer
                    job.setMapperClass(PageRankMapper.class);
                    job.setReducerClass(PageRankReducer.class);
                    //使用了新的输入格式化类
                    job.setInputFormatClass(KeyValueTextInputFormat.class);
                    //设置读取数据的路径(第一次)
                    Path inputPath = new Path("/lzj/pagerank/input/");
                    //第二次之后读取数据,就是前一次的结果
                    if (runCount > 1) {
                        inputPath = new Path("/lzj/pagerank/output/pr" + (runCount - 1));
                    }
                    //读取数据的路径
                    FileInputFormat.addInputPath(job, inputPath);
                    //本次输出路径
                    Path outpath = new Path("/lzj/pagerank/output/pr" + runCount);
                    if (fs.exists(outpath)) {
                        fs.delete(outpath, true);
                    }
                    //设置输出路径
                    FileOutputFormat.setOutputPath(job, outpath);
                    //提交任务,并获取本次任务是否成功
                    boolean flag = job.waitForCompletion(true);
                    if (flag) {
                        System.out.println("--------------------------success." + runCount);
                        //收敛值的和
                        long sum = job.getCounters().findCounter(Mycounter.my).getValue();
                        System.out.println(sum);
                        double avgConvergence = sum / 4000.0;
                        //如果平均值达到收敛指标，停止循环
                        if (avgConvergence < convergence) {
                            break;
                        }
                    }
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }
    
        /**
         * @author Administrator
         */
        static class PageRankMapper extends Mapper<Text, Text, Text, Text> {
    
            protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
                //如果是第一次读取文件，那么需要给PR设置默认值1
                int runCount = context.getConfiguration().getInt("runCount", 1);
                //以空格切分当前行，第一个空格前为KEY，其余数据为Value
                String page = key.toString();
                //定义对象Node
                Node node = null;
                //判断是否为第一次加载,将数据封装成一个对象{pr,子连接}
                if (runCount == 1) {
                    node = Node.fromMR("1.0", value.toString());
                } else {
                    node = Node.fromMR(value.toString());
                }
                //传递老的pr值和对应的页面关系 A:1.0	B	D
                context.write(new Text(page), new Text(node.toString()));
                //开始计算每个节点本次应得的pr值
                if (node.containsAdjacentNodes()) {
                    //每个节点的pr=当前页面pr/出链的数量
                    double outValue = node.getPageRank() / node.getAdjacentNodeNames().length;
                    //开始写出子节点和pr值
                    for (int i = 0; i < node.getAdjacentNodeNames().length; i++) {
                        String outPage = node.getAdjacentNodeNames()[i];
                        //页面A投给谁，谁作为key，val是票面值，票面值为：每个节点的pr
                        context.write(new Text(outPage), new Text(outValue + ""));
                    }
                }
            }
        }
    
        /**
         * @author Administrator
         */
        static class PageRankReducer extends Reducer<Text, Text, Text, Text> {
            protected void reduce(Text key, Iterable<Text> iterable, Context context) throws IOException, InterruptedException {
                //相同的key为一组
    
                //页面对应关系及老的pr值
                //B:1.0 C
                //新的投票值
                //B:0.5
                //B:0.5
                //B:0.5
                //最终写出的结果
                //B 1.5 C
    
                //定义变量存放新的PR值(阻尼之前)
                double sum = 0.0;
    
                Node sourceNode = null;
                for (Text i : iterable) {
                    //创建新的节点
                    Node node = Node.fromMR(i.toString());
                    //判断是老的映射关系还是新的PR值
                    if (node.containsAdjacentNodes()) {
                        sourceNode = node;
                    } else {
                        sum = sum + node.getPageRank();
                    }
                }
    
                // 4为页面总数
                double newPR = (0.15 / 4.0) + (0.85 * sum);
                System.out.println(key + "*********** new pageRank value is " + newPR);
    
                //把新的pr值和计算之前的pr比较
                double d = newPR - sourceNode.getPageRank();
                //保留四位有效数字,然后取绝对值
                int j = Math.abs((int) (d * 1000.0));
                context.getCounter(Mycounter.my).increment(j);
    
                //将当前网站的PR值写出
                sourceNode.setPageRank(newPR);
                context.write(key, new Text(sourceNode.toString()));
            }
        }
    
    }
    
    class Node {
        //成员变量
        private double pageRank = 1.0;
        //出链的节点名字
        private String[] adjacentNodeNames;
        //分隔符
        public static final char fieldSeparator = '\t';
    
        public double getPageRank() {
            return pageRank;
        }
    
        public Node setPageRank(double pageRank) {
            this.pageRank = pageRank;
            return this;
        }
    
        public String[] getAdjacentNodeNames() {
            return adjacentNodeNames;
        }
    
        public Node setAdjacentNodeNames(String[] adjacentNodeNames) {
            this.adjacentNodeNames = adjacentNodeNames;
            return this;
        }
    
        public boolean containsAdjacentNodes() {
            return adjacentNodeNames != null && adjacentNodeNames.length > 0;
        }
    
        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append(pageRank);
    
            if (getAdjacentNodeNames() != null) {
                sb.append(fieldSeparator).append(StringUtils.join(getAdjacentNodeNames(), fieldSeparator));
            }
            return sb.toString();
        }
    
        /**
         * @param value 0.3 B	D
         * @return
         * @throws IOException
         */
        public static Node fromMR(String value) throws IOException {
            //按照分隔符切分数据
            String[] parts = StringUtils.splitPreserveAllTokens(value, fieldSeparator);
            //如果切分后小于一块，说明少了PR值和映射关系
            if (parts.length < 1) {
                throw new IOException("Expected 1 or more parts but received " + parts.length);
            }
            //创建节点对象
            Node node = new Node().setPageRank(Double.valueOf(parts[0]));
            //如果大于1说明有子节点，
            if (parts.length > 1) {
                node.setAdjacentNodeNames(Arrays.copyOfRange(parts, 1, parts.length));
            }
            //返回节点
            return node;
        }
    
        public static Node fromMR(String v1, String v2) throws IOException {
            return fromMR(v1 + fieldSeparator + v2);
        }
    }
    ```

### TFIDF

![1593366314236](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593366314236.png)

- TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权技术。

- TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度

  - 字词的重要性随着它在文件中出现的次数成正比增加
  - 但同时会随着它在语料库中出现的频率成反比下降、

- 日常应用

  - 作为文件与用户查询之间相关程度的度量或评级

    - 搜索：王者

      »   继续搜索：王者荣耀

      »   继续搜索：王者荣耀 露娜

      »   继续搜索：王者荣耀 露娜 连招

  - 用户通过调整字词来缩小范围

    •   每个字词都有对应出现的页面

    •   通过字词数量缩小范围

    •   最终通过字词对于页面的权重来进行排序

- TF

  - term frequency--词频
  - 指的是某一个给定的词语在一份给定的文件中出现的次数
  - 一篇文章中，一个词语出现的次数越多说明越重要
  - 但是还要考虑整篇文章的词的总数，单个词所占的比例,综合来看比例越高越重要
    - 刘亦菲真好看
    - 刘亦菲刘亦菲真好看
    - 刘亦菲刘亦菲刘亦菲柳岩柳岩柳岩柳岩柳岩柳岩柳岩柳岩柳岩柳岩真好看
  - 计算公式
    - ![1593365895413](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593365895413.png)
    - 分子ni,j是该词在文件中的出现次数
    - 分母则是在文件中所有字词的出现次数之和

- IDF

  - inverse document frequency--逆向文件频率
  - 由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。
  - 多篇文章中，一个词语出现的次数越多反而越不重要
  - 计算公式
    - ![1593366083370](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593366083370.png)
    - 分子 |D|语料库中的文件总数
    - 分母表示包含指定词语文件的数目
    - 一般计算时分母数目会加1，防止出现分母为0的错误

- TF-IDF

  - 某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF
  - 倾向于过滤掉常见的词语，保留重要的词语
  - 计算公式
    - ![1593366155406](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/1593366155406.png)





![image-20201106001507751](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/image-20201106001507751.png)







## 作业

### 豆瓣电影

- 来自豆瓣的电影数据集（约有1万多条数据）
  其中包括电影名称，种类，评分，导演，编剧，演员，投票计数，国家，电影票房，版本 等。
  title,types,score,drector,writer,actor,account,country,label,flim_version
- 1.要求大家用MapReduce按照类型 地区 年代  计算每种电影的显示
- 2.每个演员参演电影的TOP 
- 3.自己去设想下如何实现  电影推荐功能？

案例数据：

武状元苏乞儿,"喜剧, 动作, 古装",7.5,陈嘉上 Gordon Chan,"陈建忠 John Chan Kin-Chung, 陈嘉上 Gordon Chan","周星驰, 张敏, 吴孟达, 徐少强, 林威, 陈百祥, 郑丹瑞, 苑琼丹, 陈慧仪",160556,香港,120.417,3



- ![image-20201110164546959](003_%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81.assets/image-20201110164546959.png)

  

# B spark



# C flink